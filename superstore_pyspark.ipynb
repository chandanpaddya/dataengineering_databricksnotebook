{"cells":[{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/s3data\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/s3data/Superstore.csv</td><td>Superstore.csv</td><td>2287742</td></tr><tr><td>dbfs:/mnt/s3data/returns.csv</td><td>returns.csv</td><td>16019</td></tr></tbody></table></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# File location and type\nfile_location = \"dbfs:/mnt/s3data/Superstore.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_superstore = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n#display(df_superstore.printSchema())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["file_location = \"dbfs:/mnt/s3data/returns.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf_returns = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location) \\\n  .dropDuplicates()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# join df_superstore and df_returns\ndf_superstore_merge=df_superstore.join(df_returns,\"Order ID\",how=\"left\")\n#display(df_superstore_merge)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Clean Column header remove spaces and special character\nfrom pyspark.sql.types import DateType,IntegerType\nfrom pyspark.sql.functions import *\nexprs = [col(column).alias(column.replace(' ', '_')) for column in df_superstore_merge.columns]\nprint(exprs)\ndf_superstore_cln=df_superstore_merge.select(*exprs)\ndf_superstore_cln=df_superstore_cln.withColumnRenamed(\"Sub-Category\", \"Sub_Category\")\\\n       .withColumnRenamed(\"Country/Region\", \"Country_Region\") \\\n       .filter(col(\"Country_Region\")==\"United States\")\n#display(df_superstore_cln)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Column&lt;b&#39;Order ID AS `Order_ID`&#39;&gt;, Column&lt;b&#39;Row ID AS `Row_ID`&#39;&gt;, Column&lt;b&#39;Order Date AS `Order_Date`&#39;&gt;, Column&lt;b&#39;Ship Date AS `Ship_Date`&#39;&gt;, Column&lt;b&#39;Ship Mode AS `Ship_Mode`&#39;&gt;, Column&lt;b&#39;Customer ID AS `Customer_ID`&#39;&gt;, Column&lt;b&#39;Customer Name AS `Customer_Name`&#39;&gt;, Column&lt;b&#39;Segment AS `Segment`&#39;&gt;, Column&lt;b&#39;Country/Region AS `Country/Region`&#39;&gt;, Column&lt;b&#39;City AS `City`&#39;&gt;, Column&lt;b&#39;State AS `State`&#39;&gt;, Column&lt;b&#39;Postal Code AS `Postal_Code`&#39;&gt;, Column&lt;b&#39;Region AS `Region`&#39;&gt;, Column&lt;b&#39;Product ID AS `Product_ID`&#39;&gt;, Column&lt;b&#39;Category AS `Category`&#39;&gt;, Column&lt;b&#39;Sub-Category AS `Sub-Category`&#39;&gt;, Column&lt;b&#39;Product Name AS `Product_Name`&#39;&gt;, Column&lt;b&#39;Sales AS `Sales`&#39;&gt;, Column&lt;b&#39;Quantity AS `Quantity`&#39;&gt;, Column&lt;b&#39;Discount AS `Discount`&#39;&gt;, Column&lt;b&#39;Profit AS `Profit`&#39;&gt;, Column&lt;b&#39;Returned AS `Returned`&#39;&gt;]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["#Convert Date column and calculate duration\ndf_superstore_cln=df_superstore_cln.withColumn(\"OrderDateClean\",to_date(col(\"Order_Date\"), \"MM/dd/yyyy\")).withColumn(\"ShipDateClean\",to_date(col(\"Ship_Date\"), \"MM/dd/yyyy\")).withColumn(\"duration\",datediff(col(\"ShipDateClean\"),col(\"OrderDateClean\")))\n#display(df_superstore_cln)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Aggegate and calcuate Sales, Qty, Avg Duration and Uniqe Customer by state, Category, sub cat and ship mode grain\ndf_superstore_sales=df_superstore_cln.groupby(\"State\",\"Category\",\"Sub_Category\",\"Ship_Mode\").agg({'Sales':'sum','Quantity':'sum','duration':'avg','Order_ID':'count'})\ndf_uniq_customer=df_superstore_cln.groupby(\"State\",\"Category\",\"Sub_Category\",\"Ship_Mode\").agg(countDistinct(\"Customer_ID\"))\n#df_superstore_agg=df_superstore_sales.join(df_uniq_customer,(df_superstore_sales.State == df_uniq_customer.State) & (df_superstore_sales.Category == df_uniq_customer.Category) & (df_superstore_sales.Sub_Category == df_uniq_customer.Sub_Category) & (df_superstore_sales.Ship_Mode == df_uniq_customer.Ship_Mode))\ndf_superstore_agg=df_superstore_sales.join(df_uniq_customer,[\"State\",\"Category\",\"Sub_Category\",\"Ship_Mode\"])\n#display(df_superstore_agg)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Prepare Final Dataframe(rename column name)\ndf_superstore_final=df_superstore_agg.select(\"State\",\"Category\",\"Sub_Category\",\"Ship_Mode\",round(col(\"avg(duration)\"),2).alias(\"Avg_Duration\"),col(\"count(Order_ID)\").alias(\"UniqUserCountLong\"),round(col(\"sum(Sales)\"),2).alias(\"SalesAmt\"),round(col(\"sum(Quantity)\"),2).alias(\"Quantity\"))\n\ndf_superstore_final=df_superstore_final.withColumn(\"UniqUserCount\", df_superstore_final[\"UniqUserCountLong\"].cast(IntegerType())).drop(\"UniqUserCountLong\")\ndf_superstore_final.printSchema()\n#display(df_superstore_final)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- State: string (nullable = true)\n-- Category: string (nullable = true)\n-- Sub_Category: string (nullable = true)\n-- Ship_Mode: string (nullable = true)\n-- Avg_Duration: double (nullable = true)\n-- SalesAmt: double (nullable = true)\n-- Quantity: double (nullable = true)\n-- UniqUserCount: integer (nullable = false)\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Load into a Table\nspark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n\nsuperstore_table_name = \"superstore_curated\"\n\ndf_superstore_final.write.format(\"parquet\").mode('overwrite').saveAsTable(superstore_table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9}],"metadata":{"name":"superstore","notebookId":1963133460410801},"nbformat":4,"nbformat_minor":0}
